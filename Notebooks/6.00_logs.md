# Notebooks v6

Self2self denoising with dropout for cryoET. Based on https://openaccess.thecvf.com/content_CVPR_2020/papers/Quan_Self2Self_With_Dropout_Learning_Self-Supervised_Denoising_From_Single_Image_CVPR_2020_paper.pdf

## 6.00 Dataloader

The dataset consists of subtomograms of shape [m, s, s, s] where m is the number of Bernoulli samples and s is the subtomogram side length.

*... we do not need to create the whole dataset of Bernoulli sampled instances in advance but just enable dropout without energy scaling on the input layer and pass the copies of the input noisy images to the NN at each iteration.* 

Think how to implement:
*... data augmentation is also used in the implementation by flipping the input image horizontally, vertically and diagonally.*

## 6.01 Model+loss

For the model, we use an input tensor of shape [batch_size, bernoulli_sample_size, subtomo_side, subtomo_side, subtomo_side] and it just follows the structure of the self2self paper, but maybe I could use a slightly smaller model.

~~The loss is currently the MSEloss summed over all dimensions, but only taking into account the points that were initially dropped out and predicted by the network. We compare the prediction against the real values of the pixels in the (1-bernoulli_mask) set of the tomogram.~~

The loss is the L2 norm across pixel values that were masked and predicted by the network, then we sum that along bernoulli samples and finally take the average over batches to prevent unwanted effects of incomplete batches that appear when just summing.

## 6.02 Training

I don't understand why there's a sudden drop in the training loss at the end of each training step, regardless of wether I shuffle the batches or not in the dataloader.

Now I'm thinking it is because the last batch has half of the batched values (4 instead of 8) due to the size of the dataset (500). **This was indeed the problem.**

## 6.03 Prediction

The network is not denoising the raw tomogram. 

I think the problem might be that it is wrong to set up the number of channels to be corresponding to the number of Bernoulli tomogram samples, since then I get additional weights and biases that are optimized for the samples, which is not what I want. I want a set of weights and biases for the image only, which has one channel, and then I want to take several Bernoulli samples in order to compute the loss on the network's predicted, but originally missing, pixels.

## Issues

~~My predictions are skewed in the [0, 1] range with most values gathered close to 0. This means that half of the variation of the data is not present in my predictions, which looks weird. Current setting is clipping and standardizig the data, no data augmentation. Log of the things I've tried to solve the problem:~~
    - Maybe pointed masks do not work to help on the denoising, I will only use volumetric masks. (tryout_model_logs/version_16)
    - **Compare Vmask (tryout_model_logs/version_21) with Pmask (tryout_model_logs/version_22)**: loss goes down way faster using the Pmask and the results look more consistent. The histogram distribution also looks a little bit more spread. I still think that combining the Vmask and Pmask should yield better results, but need to explore further.
    - **Having a LeakyReLu just before the output yielded heavily skewed predictions. Turning that off in the last layer helped training. This was the issue.**

Seems that total variation is not really helping, I am taking it over the prediction over the masked values. Using total variation yields very weird results, the denoised images look like "ghosts" and the variance of the pixels is completely squashed.
The Vmask seems to be generating some issues with the training and its hard to know if its helping. But usually the results are a little bit more blurry.

## Current state of Trainer

- Loss is the L2 loss.
- Data augmentation is random rotations with at least 0.5 probability (each axis has 0.5 independent chance of rotation)
- Vmask with probability 0.1



## Things to try next

- Volumetric blind spots ✓
- Use only certain patches for training:
    - What is the minimum number of patches we could use to achieve "good" denoising?
    - What would be a good way to define the patches?
- Modify the loss to include sparsity components (Total variation) ✓
    - I think total variation should be measured in the modified full image with the masked values replaced with the predictions 
- Train using deconvolved tomogram, or a mixture of the raw and deconvolved
- Include data augmentation
    - Random rotations ✓
    - Different masking strategies: pointed and volumetric ✓ (does this makes sense?)
    - 
- Think of a way to use more than one bernoulli sample. Maybe make, e.g. 3 samples, then flatten the array on the first dimension when taking batches.
