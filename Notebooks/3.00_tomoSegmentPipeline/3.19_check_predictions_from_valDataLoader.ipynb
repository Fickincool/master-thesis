{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cdfe4e-48ad-48d0-aaa4-4d45354692a3",
   "metadata": {},
   "source": [
    "**P** I think it is not the same to predict all validation batches across patches using the dataloader as to predict all validation patches with the method I'm using in Showcase results. \n",
    "\n",
    "**Answer** it was not, predicting on validation patches using my method was not including patch normalization.\n",
    "\n",
    "**P** How is it different to predict on the validation set with my method, i.e. explicitely loading the patches and making the predictions using the centers, versus the `validate` method from pytorch?\n",
    "\n",
    "**Answer** The difference lays in the batch sizes, using `validate` method uses the same batch size to do the validation as it is used for training, and this yields different results than doing the validation the way I'm doing it because I'm doing it with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e4d8ce-de27-495c-9518-9928e0fb75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomoSegmentPipeline import dataloader as dl\n",
    "from tomoSegmentPipeline.utils import setup\n",
    "from tomoSegmentPipeline.dataloader import to_categorical, transpose_to_channels_first, tomoSegment_dummyDataset, tomoSegment_dataset\n",
    "from tomoSegmentPipeline.training import Train\n",
    "from tomoSegmentPipeline.showcaseResults import (predict_fullTomogram, load_model, load_tomoData, Tversky_index, Tversky_loss,\n",
    "                                        fullTomogram_modelComparison, make_comparison_plot, write_comparison_gif, save_classPred)\n",
    "\n",
    "from tomoSegmentPipeline.model import DeepFinder_model\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "PARENT_PATH = setup.PARENT_PATH\n",
    "\n",
    "train_tomos = ['tomo02', 'tomo03', 'tomo17']\n",
    "concat_train_ids = sorted([s.replace('tomo', '') for s in train_tomos])\n",
    "concat_train_ids = '-'.join(concat_train_ids)\n",
    "\n",
    "val_tomos = ['tomo32', 'tomo10']\n",
    "concat_val_ids = sorted([s.replace('tomo', '') for s in val_tomos])\n",
    "concat_val_ids = '-'.join(concat_val_ids)\n",
    "\n",
    "test_tomos = ['tomo38', 'tomo04']\n",
    "concat_test_ids = sorted([s.replace('tomo', '') for s in test_tomos])\n",
    "concat_test_ids = '-'.join(concat_test_ids)\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bc953-8e88-49ea-94e5-05e13f857663",
   "metadata": {},
   "source": [
    "# Load models and reference values for validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57e8a19-93d2-46e3-8974-4b8e4098c7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model_file</th>\n",
       "      <th>input_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>best_val_loss_epoch</th>\n",
       "      <th>associated_val_class1_dice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>/home/haicu/jeronimo.carvajal/Thesis/data/mode...</td>\n",
       "      <td>cryoCARE+isoNET</td>\n",
       "      <td>438 out of 1000</td>\n",
       "      <td>in84</td>\n",
       "      <td>lr0.000100</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.092645</td>\n",
       "      <td>0.916506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                         model_file  \\\n",
       "0  Baseline  /home/haicu/jeronimo.carvajal/Thesis/data/mode...   \n",
       "\n",
       "        input_type           epochs patch_size          lr version  \\\n",
       "0  cryoCARE+isoNET  438 out of 1000       in84  lr0.000100      v1   \n",
       "\n",
       "   best_val_loss_epoch  associated_val_class1_dice  \n",
       "0             0.092645                    0.916506  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_path = PARENT_PATH + 'data/model_logs/models_1/logs/BaselineModel/'\n",
    "logs_path = Path(logs_path)\n",
    "\n",
    "model_info = []\n",
    "\n",
    "# logdir_path = '/home/haicu/jeronimo.carvajal/Thesis/data/model_logs/models_1/logs/BaselineModel/cryoCARE/train02-03-17/version_2/'\n",
    "# batch_size = 32\n",
    "logdir_path = '/home/haicu/jeronimo.carvajal/Thesis/data/model_logs/models_1/logs/BaselineModel/cryoCARE+isoNET/train02-03-17/version_1/'\n",
    "batch_size = 22\n",
    "\n",
    "model_file = glob(os.path.join(logdir_path, '*.model'))\n",
    "\n",
    "model_file = model_file[0]\n",
    "\n",
    "model_file_split = model_file.split('/')\n",
    "\n",
    "input_type = model_file_split[-4]\n",
    "\n",
    "name, epochs, patch_size, lr, version = model_file_split[-1].split('_')\n",
    "epochs = int(epochs.replace('ep', ''))\n",
    "version = 'v'+version.replace('.model', '')\n",
    "\n",
    "events_path = glob(os.path.join(logdir_path, 'events.*'))[0]\n",
    "event_acc = EventAccumulator(events_path)\n",
    "event_acc.Reload()\n",
    "\n",
    "_, step_nums, values_valLoss = zip(*event_acc.Scalars('hp/val_loss_epoch'))\n",
    "best_val_loss_epoch = np.min(values_valLoss)\n",
    "best_val_loss_epoch_idx = np.argmin(values_valLoss) #index starts count at 0\n",
    "\n",
    "effective_epochs = len(values_valLoss)\n",
    "\n",
    "_, _, values_dice = zip(*event_acc.Scalars('hp/val_dice_epoch'))\n",
    "_, _, values_trainLoss = zip(*event_acc.Scalars('hp/train_loss_epoch'))\n",
    "\n",
    "associated_val_class1_dice = float(values_dice[best_val_loss_epoch_idx])\n",
    "associated_train_loss_epoch = float(values_trainLoss[best_val_loss_epoch_idx])\n",
    "\n",
    "epochs_str = \"%i out of %i\" %(effective_epochs, 1000)\n",
    "\n",
    "model_info.append([name, model_file, input_type, epochs_str, patch_size, lr, version, best_val_loss_epoch, associated_val_class1_dice])\n",
    "\n",
    "df_model = pd.DataFrame(model_info, columns=['name', 'model_file', 'input_type', 'epochs', 'patch_size', 'lr', 'version', 'best_val_loss_epoch',\n",
    "                                         'associated_val_class1_dice'])\n",
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a199712-0aa1-484b-8316-1fe0e7a1ce26",
   "metadata": {},
   "source": [
    "# Predict directly on the validation patches using the model and then averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f51dd-11c6-42c5-b904-5b07e0cca7b7",
   "metadata": {},
   "source": [
    "**Observation** Model from checkpoint and model from state dict yield the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b871f6d-1824-43fd-bbde-8c62a2086679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = int(patch_size.replace('in', ''))\n",
    "\n",
    "paths_valData, paths_valTarget = setup.get_paths(val_tomos, input_type)\n",
    "\n",
    "my_dataset = dl.tomoSegment_dataset(paths_valData, paths_valTarget, dim_in=dim_in, Ncl=3, Lrnd=0, augment_data=False)\n",
    "# len(my_dataset)\n",
    "\n",
    "model = load_model(df_model.model_file.values[0], Nclass=2)\n",
    "\n",
    "# loss_fn = Tversky_loss()\n",
    "# model = DeepFinder_model(2, loss_fn, 20, 0, None) \n",
    "# model = model.load_from_checkpoint(checkpoint_path=ckpt_file).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8426de73-21c1-4e84-85fa-4a5fe2ab9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, instance in enumerate(my_dataset):\n",
    "    idx, instance\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f90918ad-e8a8-4d54-84ec-992c770b4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nclass_data = 3\n",
    "Nclass_model = 2\n",
    "z, y, x = 3*[dim_in]\n",
    "\n",
    "loss = Tversky_loss()\n",
    "\n",
    "dice_average = []\n",
    "dice_class_avg = []\n",
    "dice1_avg = []\n",
    "val_loss_avg = []\n",
    "\n",
    "for instance in my_dataset:\n",
    "    patch, target = instance\n",
    "    patch = torch.as_tensor(patch).unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(patch).to(\"cpu\")\n",
    "        \n",
    "    y_true = torch.zeros((1, Nclass_data, z, y, x))\n",
    "    y_true[0, :, :, :, :] = target\n",
    "    y_true = y_true.to(\"cpu\")\n",
    "    \n",
    "    dice_by_class = Tversky_index(y_pred.to(\"cuda\"), y_true.to(\"cuda\")).to('cpu')\n",
    "    dice = Nclass_model - loss(y_pred.to(\"cuda\"), y_true.to(\"cuda\")).to('cpu')\n",
    "    dice_average.append(float(dice))\n",
    "    dice_class_avg.append(dice_by_class)\n",
    "    dice1_avg.append(float(dice_by_class[1]))\n",
    "    \n",
    "    val_loss = loss(y_pred.to(\"cuda\"), y_true.to(\"cuda\")).to('cpu')\n",
    "    val_loss_avg.append(float(val_loss))\n",
    "    \n",
    "dice_average = np.mean(dice_average)\n",
    "avg_loss = np.mean(val_loss_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "274807e5-f8b4-47fe-8fb2-bd4bcc69c99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model_file</th>\n",
       "      <th>input_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>best_val_loss_epoch</th>\n",
       "      <th>associated_val_class1_dice</th>\n",
       "      <th>direct_val_loss</th>\n",
       "      <th>direct_val_class1_dice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>/home/haicu/jeronimo.carvajal/Thesis/data/mode...</td>\n",
       "      <td>cryoCARE+isoNET</td>\n",
       "      <td>438 out of 1000</td>\n",
       "      <td>in84</td>\n",
       "      <td>lr0.000100</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.092645</td>\n",
       "      <td>0.916506</td>\n",
       "      <td>0.347581</td>\n",
       "      <td>0.665177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                         model_file  \\\n",
       "0  Baseline  /home/haicu/jeronimo.carvajal/Thesis/data/mode...   \n",
       "\n",
       "        input_type           epochs patch_size          lr version  \\\n",
       "0  cryoCARE+isoNET  438 out of 1000       in84  lr0.000100      v1   \n",
       "\n",
       "   best_val_loss_epoch  associated_val_class1_dice  direct_val_loss  \\\n",
       "0             0.092645                    0.916506         0.347581   \n",
       "\n",
       "   direct_val_class1_dice  \n",
       "0                0.665177  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['direct_val_loss'] = avg_loss\n",
    "df_model['direct_val_class1_dice'] = np.mean(dice1_avg)\n",
    "# this results do not look good\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd92940-b26d-4844-ae9d-02ac3b9314b1",
   "metadata": {},
   "source": [
    "## Predict using the Dataloader with the proper batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2b9d8a-fbab-4770-a9e7-6b021977d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice1_avg = []\n",
    "val_loss_avg = []\n",
    "\n",
    "val_loader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=1)\n",
    "\n",
    "loss = Tversky_loss()\n",
    "\n",
    "for patch, target in val_loader:\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(patch)\n",
    "        \n",
    "    y_true = target.to('cuda')\n",
    "    \n",
    "    dice_by_class = Tversky_index(y_pred, y_true).to('cpu')\n",
    "    dice1_avg.append(float(dice_by_class[1]))\n",
    "    \n",
    "    val_loss = loss(y_pred, y_true).to('cpu')\n",
    "    val_loss_avg.append(float(val_loss))\n",
    "    \n",
    "dice1_avg = np.mean(dice1_avg)\n",
    "avg_loss = np.mean(val_loss_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecefb56-63d0-47f4-92fc-1fa1dd4a66ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model_file</th>\n",
       "      <th>input_type</th>\n",
       "      <th>epochs</th>\n",
       "      <th>patch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>best_val_loss_epoch</th>\n",
       "      <th>associated_val_class1_dice</th>\n",
       "      <th>direct_val_loss</th>\n",
       "      <th>direct_val_class1_dice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>/home/haicu/jeronimo.carvajal/Thesis/data/mode...</td>\n",
       "      <td>cryoCARE+isoNET</td>\n",
       "      <td>438 out of 1000</td>\n",
       "      <td>in84</td>\n",
       "      <td>lr0.000100</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.092645</td>\n",
       "      <td>0.916506</td>\n",
       "      <td>0.089028</td>\n",
       "      <td>0.922572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                         model_file  \\\n",
       "0  Baseline  /home/haicu/jeronimo.carvajal/Thesis/data/mode...   \n",
       "\n",
       "        input_type           epochs patch_size          lr version  \\\n",
       "0  cryoCARE+isoNET  438 out of 1000       in84  lr0.000100      v1   \n",
       "\n",
       "   best_val_loss_epoch  associated_val_class1_dice  direct_val_loss  \\\n",
       "0             0.092645                    0.916506         0.089028   \n",
       "\n",
       "   direct_val_class1_dice  \n",
       "0                0.922572  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['direct_val_loss'] = avg_loss\n",
    "df_model['direct_val_class1_dice'] = np.mean(dice1_avg)\n",
    "# this results do not look good\n",
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3ea1d-46b6-4a10-940b-fe6d9dc9d75b",
   "metadata": {},
   "source": [
    "# Predict using the `validate` method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c09a52d-14f0-41c3-92b9-f3b10b63f456",
   "metadata": {},
   "source": [
    "**Conclusion** The results differ but they are closer to what we see in the logs. The problem seems to be the different batch sizes. My method takes batch sizes of 1 and then averages the loss, the validation takes the batch size specified when the trainer is invoked in the `launch` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbcfd41e-c440-45e2-ba55-90003b9d99de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Restoring states from the checkpoint path at /home/haicu/jeronimo.carvajal/Thesis/data/model_logs/models_1/logs/BaselineModel/cryoCARE+isoNET/train02-03-17/version_1/checkpoints/epoch=437-step=1751.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from checkpoint at /home/haicu/jeronimo.carvajal/Thesis/data/model_logs/models_1/logs/BaselineModel/cryoCARE+isoNET/train02-03-17/version_1/checkpoints/epoch=437-step=1751.ckpt\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3\n",
      "initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 3 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 3/3 [00:09<00:00,  2.65s/it]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'hp/val_acc': 0.7083588242530823,\n",
      " 'hp/val_dice': 0.9215565323829651,\n",
      " 'hp/val_loss': 0.09030797332525253}\n",
      "--------------------------------------------------------------------------------\n",
      "                                                         \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'hp/val_loss': 0.09030797332525253,\n",
       "  'hp/val_acc': 0.7083588242530823,\n",
       "  'hp/val_dice': 0.9215565323829651}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_in = int(patch_size.replace('in', ''))\n",
    "\n",
    "paths_valData, paths_valTarget = setup.get_paths(val_tomos, input_type)\n",
    "\n",
    "my_dataset = dl.tomoSegment_dataset(paths_valData, paths_valTarget, dim_in=dim_in, Ncl=3, Lrnd=0, augment_data=False)\n",
    "val_loader = DataLoader(my_dataset, batch_size=22, shuffle=True, pin_memory=True, num_workers=1)\n",
    "\n",
    "loss_fn = Tversky_loss()\n",
    "\n",
    "model = DeepFinder_model(2, loss_fn, 1e-4, 0, None) \n",
    "\n",
    "trainer = pl.Trainer(gpus=3)\n",
    "\n",
    "aux_model_name = model_file.split('/')[-1]\n",
    "ckpt_file = model_file.replace(aux_model_name, 'checkpoints/')\n",
    "ckpt_file = glob(ckpt_file+'*')[0]\n",
    "\n",
    "trainer.validate(model, dataloaders=val_loader, ckpt_path=ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c0f1879-4741-4e72-80f0-2dcee2422e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Restoring states from the checkpoint path at /home/haicu/jeronimo.carvajal/Thesis/data/model_logs/models_1/logs/BaselineModel/cryoCARE+isoNET/train02-03-17/version_1/checkpoints/epoch=437-step=1751.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Loaded model weights from checkpoint at /home/haicu/jeronimo.carvajal/Thesis/data/model_logs/models_1/logs/BaselineModel/cryoCARE+isoNET/train02-03-17/version_1/checkpoints/epoch=437-step=1751.ckpt\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/3\n",
      "initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 3 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 51/51 [00:14<00:00,  2.69it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'hp/val_acc': 0.7083588242530823,\n",
      " 'hp/val_dice': 0.6669571995735168,\n",
      " 'hp/val_loss': 0.3458147644996643}\n",
      "--------------------------------------------------------------------------------\n",
      "                                                           \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'hp/val_loss': 0.3458147644996643,\n",
       "  'hp/val_acc': 0.7083588242530823,\n",
       "  'hp/val_dice': 0.6669571995735168}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_in = int(patch_size.replace('in', ''))\n",
    "\n",
    "paths_valData, paths_valTarget = setup.get_paths(val_tomos, input_type)\n",
    "\n",
    "my_dataset = dl.tomoSegment_dataset(paths_valData, paths_valTarget, dim_in=dim_in, Ncl=3, Lrnd=0, augment_data=False)\n",
    "val_loader = DataLoader(my_dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=1)\n",
    "\n",
    "loss_fn = Tversky_loss()\n",
    "\n",
    "model = DeepFinder_model(2, loss_fn, 1e-4, 0, None) \n",
    "\n",
    "trainer = pl.Trainer(gpus=3)\n",
    "\n",
    "aux_model_name = model_file.split('/')[-1]\n",
    "ckpt_file = model_file.replace(aux_model_name, 'checkpoints/')\n",
    "ckpt_file = glob(ckpt_file+'*')[0]\n",
    "\n",
    "trainer.validate(model, dataloaders=val_loader, ckpt_path=ckpt_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8a3baf7-7cbe-4ec4-a889-145780af16c7",
   "metadata": {},
   "source": [
    "{'hp/val_loss': 0.3475813567638397,\n",
    "  'hp/val_acc': 0.7098331451416016,\n",
    "  'hp/val_dice': 0.6651765704154968}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507b0fc-8357-464c-aef4-70affdd6fb61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
