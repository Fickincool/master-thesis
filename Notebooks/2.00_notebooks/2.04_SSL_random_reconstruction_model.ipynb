{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a405a2e8-8542-4538-b6e6-eccb04b1e22f",
   "metadata": {},
   "source": [
    "Train DeepFinder model with pretrained weights. The weights are obtained from a modified DeepFinder model trained to reconstruct the original images from input patches with randomly masked sets of pixels. The models use all center crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52360c9-4620-4cc6-b327-0c54ef20e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of tomo IDs to indices:\n",
      "[('tomo02', 0), ('tomo03', 1), ('tomo04', 2), ('tomo10', 3), ('tomo17', 4), ('tomo32', 5), ('tomo38', 6)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jeroHelper.setupUtils import append_deepfinder_path, get_tomo_indices, PARENT_PATH\n",
    "append_deepfinder_path()\n",
    "\n",
    "tomo_ids, tomo_idx = get_tomo_indices()\n",
    "print('\\n')\n",
    "\n",
    "from jeroHelper.coordGen import make_random_xml_objlist_from_crops, make_xml_objlist_from_crops\n",
    "from jeroHelper.trainHelper import make_trainer\n",
    "\n",
    "from deepfinder.training_pylit import TargetBuilder\n",
    "from deepfinder.dataloader_pylit import DeepFinder_dataset, to_categorical, transpose_to_channels_first\n",
    "from deepfinder.model_pylit import DeepFinder_model\n",
    "import deepfinder.utils.objl as ol\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import mrcfile\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74129a9b-b83a-40be-bbf8-b1a7b8e0fb27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reconstruction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84442cc0-9460-4d39-9d8e-a07cccd7ea42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## TRAIN TOMOGRAM ######################\n",
      "Generating random object list for tomo02\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "\n",
      "\n",
      "######################## TRAIN TOMOGRAM ######################\n",
      "Generating random object list for tomo03\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "\n",
      "\n",
      "######################## TRAIN TOMOGRAM ######################\n",
      "Generating random object list for tomo04\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "\n",
      "\n",
      "######################## VALIDATION TOMOGRAM ######################\n",
      "Generating object list for tomo10\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "Total number of samples:  2001\n",
      "\n",
      "\n",
      "######################## TRAIN TOMOGRAM ######################\n",
      "Generating random object list for tomo17\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "\n",
      "\n",
      "######################## VALIDATION TOMOGRAM ######################\n",
      "Generating object list for tomo32\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "Total number of samples:  2001\n",
      "\n",
      "\n",
      "\n",
      "Train object list created at: \n",
      "/home/haicu/jeronimo.carvajal/Thesis/data/processed0/deepFinder/object_lists/random_sample_train_tomo02-03-04-17_309-618_309-618_100-350.xml\n",
      "\n",
      "Validation object list created at: \n",
      "/home/haicu/jeronimo.carvajal/Thesis/data/processed0/deepFinder/object_lists/validation_tomo10-32_309-618_309-618_100-350.xml\n"
     ]
    }
   ],
   "source": [
    "train_tomos = ['tomo02', 'tomo03', 'tomo04', 'tomo17']\n",
    "concat_train_ids = sorted([s.replace('tomo', '') for s in train_tomos])\n",
    "concat_train_ids = '-'.join(concat_train_ids)\n",
    "\n",
    "val_tomos = ['tomo32', 'tomo10']\n",
    "concat_val_ids = sorted([s.replace('tomo', '') for s in val_tomos])\n",
    "concat_val_ids = '-'.join(concat_val_ids)\n",
    "\n",
    "test_tomos = ['tomo38']\n",
    "\n",
    "crops_coords_str = '309-618_309-618_100-350'\n",
    "\n",
    "my_tomo = tomo_ids[3] \n",
    "lbl_file = PARENT_PATH+'data/processed0/nnUnet/cET_cropped/%s_merged_thr02_lbl_%s.mrc' %(my_tomo, crops_coords_str)\n",
    "\n",
    "tomo_file = PARENT_PATH+'data/processed0/nnUnet/cET_cropped/%s_bin4_denoised_0000_%s.mrc' %(my_tomo, crops_coords_str)\n",
    "\n",
    "mrc = mrcfile.open(tomo_file, mode='r')\n",
    "tomo_data = mrc.data\n",
    "mrc.close()\n",
    "\n",
    "make_random_xml_objlist_from_crops(tomo_ids, tomo_idx, crops_coords_str, 3000, 2000, 0, train_tomos, val_tomos, test_tomos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69caf0ed-8fd0-45f6-8150-a2efdd4b6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This only makes sense if ONE crop of each tomogram is used\n",
    "path_data = []\n",
    "path_target = []\n",
    "\n",
    "# For reconstruction model\n",
    "data_template_str = 'data/processed0/nnUnet/cET_cropped/%s_bin4_denoised_0000_%s.mrc'\n",
    "# irrelevant in the case of reconstruction task, left here to keep code as is\n",
    "target_template_str = 'data/processed0/tomoSegMemTV_proxyLabels/%s_bin4_denoised_0000_%s_tomoSegMemTV_lbl.mrc' \n",
    "\n",
    "for tomo_id, deepFinder_idx in zip(tomo_ids, tomo_idx):\n",
    "\n",
    "    file_data = PARENT_PATH+data_template_str %(tomo_id, crops_coords_str)\n",
    "    file_target = PARENT_PATH+target_template_str %(tomo_id, crops_coords_str)\n",
    "    \n",
    "    path_data+=[file_data]\n",
    "    path_target+=[file_target]\n",
    "    \n",
    "path_objl_train = '../data/processed0/deepFinder/object_lists/random_sample_train_tomo%s_%s.xml' %(concat_train_ids, crops_coords_str)\n",
    "# compare giving more information within this task using proxy labels (seems like it doesnt add much)\n",
    "# path_objl_train = '../data/processed0/deepFinder/object_lists/TSMTV_proxy_labels_train_tomo%s_%s.xml' %(concat_train_ids, crops_coords_str)\n",
    "\n",
    "path_objl_valid = '../data/processed0/deepFinder/object_lists/validation_tomo%s_%s.xml' %(concat_val_ids, crops_coords_str)\n",
    "\n",
    "# Load object lists:\n",
    "objl_train = ol.read_xml(path_objl_train)\n",
    "objl_valid = ol.read_xml(path_objl_valid)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "rsample_train = random.sample(objl_train, 500)\n",
    "rsample_val = random.sample(objl_valid, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0667b-a1e2-42e5-8428-967a23eb73de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting trainer for reconstruction task... Ignoring pretrained_model value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | loss_fn | MSELoss    | 0     \n",
      "1 | layer1  | Sequential | 28.6 K\n",
      "2 | layer2  | Sequential | 103 K \n",
      "3 | layer3  | Sequential | 558 K \n",
      "4 | layer4  | Sequential | 288 K \n",
      "5 | layer5  | Sequential | 96.9 K\n",
      "---------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.303     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"loss_fn\":      MSELoss()\n",
      "\"lr\":           0.0001\n",
      "\"weight_decay\": 0.0\n"
     ]
    }
   ],
   "source": [
    "tb_logdir = './logs/2.04_reconstructionModel/'\n",
    "\n",
    "trainer = make_trainer(dim_in=56, batch_size=32, lr=1e-4, epochs=600, tb_logdir=tb_logdir,\n",
    "                       model_name='2.04_reconstructionModel', reconstruction_trainer=True,\n",
    "                       pretrained_model=None)\n",
    "trainer.launch(path_data, path_target, rsample_train, rsample_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda795e-3a64-453d-9e24-09b842f5d048",
   "metadata": {},
   "source": [
    "# Pretrained model with reconstruction task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de804ed7-32d3-477d-8b45-211d3e04ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## TRAIN TOMOGRAM ######################\n",
      "Generating object list for tomo02\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "Total number of samples:  3001\n",
      "\n",
      "\n",
      "######################## VALIDATION TOMOGRAM ######################\n",
      "Generating object list for tomo10\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "Total number of samples:  2001\n",
      "\n",
      "\n",
      "######################## VALIDATION TOMOGRAM ######################\n",
      "Generating object list for tomo32\n",
      "Tomogram shape:  (250, 309, 309)\n",
      "Total number of samples:  2001\n",
      "\n",
      "\n",
      "\n",
      "Train object list created at: \n",
      "/home/haicu/jeronimo.carvajal/Thesis/data/processed0/deepFinder/object_lists/train_tomo02_309-618_309-618_100-350.xml\n",
      "\n",
      "Validation object list created at: \n",
      "/home/haicu/jeronimo.carvajal/Thesis/data/processed0/deepFinder/object_lists/validation_tomo10-32_309-618_309-618_100-350.xml\n"
     ]
    }
   ],
   "source": [
    "train_tomos = ['tomo02']\n",
    "concat_train_ids = sorted([s.replace('tomo', '') for s in train_tomos])\n",
    "concat_train_ids = '-'.join(concat_train_ids)\n",
    "\n",
    "val_tomos = ['tomo32', 'tomo10']\n",
    "concat_val_ids = sorted([s.replace('tomo', '') for s in val_tomos])\n",
    "concat_val_ids = '-'.join(concat_val_ids)\n",
    "\n",
    "test_tomos = ['tomo38']\n",
    "\n",
    "crops_coords_str = '309-618_309-618_100-350'\n",
    "\n",
    "use_proxy_labels_for_train = False\n",
    "\n",
    "make_xml_objlist_from_crops(tomo_ids, tomo_idx, crops_coords_str, 3000, 2000, 0, use_proxy_labels_for_train,\n",
    "                            train_tomos, val_tomos, test_tomos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1594d24-9b80-4f9c-9d2e-699c75c5b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This only makes sense if ONE crop of each tomogram is used\n",
    "path_data = []\n",
    "path_target = []\n",
    "\n",
    "# For pretrained model\n",
    "data_template_str = 'data/processed0/nnUnet/cET_cropped/%s_bin4_denoised_0000_%s.mrc'\n",
    "target_template_str = 'data/processed0/nnUnet/cET_cropped/%s_merged_thr02_lbl_%s.mrc'\n",
    "\n",
    "for tomo_id, deepFinder_idx in zip(tomo_ids, tomo_idx):\n",
    "\n",
    "    file_data = PARENT_PATH+data_template_str %(tomo_id, crops_coords_str)\n",
    "    file_target = PARENT_PATH+target_template_str %(tomo_id, crops_coords_str)\n",
    "    \n",
    "    path_data+=[file_data]\n",
    "    path_target+=[file_target]\n",
    "    \n",
    "path_objl_train = '../data/processed0/deepFinder/object_lists/train_tomo%s_%s.xml' %(concat_train_ids, crops_coords_str)\n",
    "path_objl_valid = '../data/processed0/deepFinder/object_lists/validation_tomo%s_%s.xml' %(concat_val_ids, crops_coords_str)\n",
    "\n",
    "# Load object lists:\n",
    "objl_train = ol.read_xml(path_objl_train)\n",
    "objl_valid = ol.read_xml(path_objl_valid)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "rsample_train = random.sample(objl_train, 500)\n",
    "rsample_val = random.sample(objl_valid, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30eb509-8c37-4572-a794-e41852751f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | loss_fn | Tversky_loss | 0     \n",
      "1 | layer1  | Sequential   | 28.6 K\n",
      "2 | layer2  | Sequential   | 103 K \n",
      "3 | layer3  | Sequential   | 558 K \n",
      "4 | layer4  | Sequential   | 288 K \n",
      "5 | layer5  | Sequential   | 96.9 K\n",
      "-----------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.304     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ncl\":           2\n",
      "\"loss_fn\":       Tversky_loss()\n",
      "\"lr\":            0.0001\n",
      "\"pretrain_type\": reconstructionTask\n",
      "\"weight_decay\":  0.0\n"
     ]
    }
   ],
   "source": [
    "tb_logdir = './logs/2.04_preTrainedReconstructionTaskModel/'\n",
    "\n",
    "trainer = make_trainer(dim_in=56, batch_size=32, lr=1e-4, epochs=600, tb_logdir=tb_logdir,\n",
    "                       model_name='2.04_preTrainedReconstructionTaskModel',\n",
    "                       reconstruction_trainer=False, \n",
    "                       pretrained_model=PARENT_PATH+'models/2.04_reconstructionModel_ep600_in56_lr0.000100_v3.model')\n",
    "# v0, v2 reconstruction model was trained on random samples -> v0, v1 preTrainedReconstructionTaskModel uses this\n",
    "# v1 reconstruction model was trained on TSMTV proxy labels -> v2, v3 preTrainedReconstructionTaskModel uses this\n",
    "\n",
    "trainer.launch(path_data, path_target, rsample_train, rsample_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903b14e-e876-4073-8049-1ac9859fc05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6b14-c29c-4e43-baf5-5d00e0ff8e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f44a6d-ed31-4488-9360-98ec333af0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac7e57-bac9-40f0-96af-2112874bb73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ac7ebc-c1c5-4900-a27e-b302631e1549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
