{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab2330d-5549-433e-b124-f3904bca6fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs of tomo IDs to indices:\n",
      "[('tomo02', 0), ('tomo03', 1), ('tomo04', 2), ('tomo10', 3), ('tomo17', 4), ('tomo32', 5), ('tomo38', 6)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from jeroHelper.setupUtils import append_deepfinder_path, get_tomo_indices, PARENT_PATH\n",
    "append_deepfinder_path()\n",
    "\n",
    "tomo_ids, tomo_idx = get_tomo_indices()\n",
    "print('\\n')\n",
    "\n",
    "from jeroHelper.coordGen import make_xml_objlist_from_crops\n",
    "from jeroHelper.trainHelper import make_trainer\n",
    "\n",
    "from deepfinder.training_pylit import TargetBuilder\n",
    "from deepfinder.dataloader_pylit import DeepFinder_dataset, to_categorical, transpose_to_channels_first\n",
    "from deepfinder.model_pylit import DeepFinder_model\n",
    "import deepfinder.utils.objl as ol\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e14cbf-1795-4a24-bc13-563e5902cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tomos = ['tomo02', 'tomo03', 'tomo04', 'tomo17']\n",
    "concat_train_ids = sorted([s.replace('tomo', '') for s in train_tomos])\n",
    "concat_train_ids = '-'.join(concat_train_ids)\n",
    "\n",
    "val_tomos = ['tomo32', 'tomo10']\n",
    "concat_val_ids = sorted([s.replace('tomo', '') for s in val_tomos])\n",
    "concat_val_ids = '-'.join(concat_val_ids)\n",
    "\n",
    "\n",
    "test_tomos = ['tomo38']\n",
    "\n",
    "crops_coords_str = '309-618_309-618_100-350'\n",
    "\n",
    "#### This only makes sense if ONE crop of each tomogram is used\n",
    "path_data = []\n",
    "path_target = []\n",
    "\n",
    "# For  baseline\n",
    "data_template_str = 'data/processed0/nnUnet/cET_cropped/%s_bin4_denoised_0000_%s.mrc'\n",
    "target_template_str = 'data/processed0/nnUnet/cET_cropped/%s_merged_thr02_lbl_%s.mrc'\n",
    "\n",
    "for tomo_id, deepFinder_idx in zip(tomo_ids, tomo_idx):\n",
    "\n",
    "    file_data = PARENT_PATH+data_template_str %(tomo_id, crops_coords_str)\n",
    "    file_target = PARENT_PATH+target_template_str %(tomo_id, crops_coords_str)\n",
    "    \n",
    "    path_data+=[file_data]\n",
    "    path_target+=[file_target]\n",
    "    \n",
    "path_objl_train = '../data/processed0/deepFinder/object_lists/train_tomo%s_%s.xml' %(concat_train_ids, crops_coords_str)\n",
    "path_objl_valid = '../data/processed0/deepFinder/object_lists/validation_tomo%s_%s.xml' %(concat_val_ids, crops_coords_str)\n",
    "\n",
    "# Load object lists:\n",
    "objl_train = ol.read_xml(path_objl_train)\n",
    "objl_valid = ol.read_xml(path_objl_valid)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "rsample_train = random.sample(objl_train, 500)\n",
    "rsample_val = random.sample(objl_valid, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d79a067-1898-49ab-a307-cb829e8e7240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params\n",
      "-----------------------------------------\n",
      "0 | loss_fn | Tversky_loss | 0     \n",
      "1 | layer1  | Sequential   | 28.6 K\n",
      "2 | layer2  | Sequential   | 103 K \n",
      "3 | layer3  | Sequential   | 558 K \n",
      "4 | layer4  | Sequential   | 288 K \n",
      "5 | layer5  | Sequential   | 96.9 K\n",
      "-----------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.304     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Ncl\":           2\n",
      "\"loss_fn\":       Tversky_loss()\n",
      "\"lr\":            0.0001\n",
      "\"pretrain_type\": None\n",
      "\"weight_decay\":  0.0\n",
      "Saving model at:  /home/jeronimo/Desktop/Master_mathematics/Thesis/models/2.00_baselineModel_ep2_in32_lr0.000100_v9.model\n",
      "[tensor(1.0307), tensor(0.9499), tensor(0.5043)] [tensor(1.0041), tensor(0.7415)]\n",
      "Trying to open file...\n"
     ]
    }
   ],
   "source": [
    "tb_logdir = './logs/2.00_baselineModel/'\n",
    "# pretrained_model = PARENT_PATH+'models/2.01_proxyLabelModel_ep1__in32_lr0.000100_v0.model'\n",
    "pretrained_model = None\n",
    "\n",
    "trainer = make_trainer(dim_in=32, batch_size=32, lr=1e-4, epochs=2, tb_logdir=tb_logdir, model_name='2.00_baselineModel',\n",
    "                       reconstruction_trainer=False, pretrained_model=pretrained_model)\n",
    "trainer.launch(path_data, path_target, rsample_train, rsample_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "898adc94-4fc4-42b7-8dd2-42dafd183162",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '/home/jeronimo/Desktop/Master_mathematics/Thesis/models/model_summary'\n",
    "\n",
    "header = \"\"\"Name,Version,Training set,# Samples training,Validation set,\n",
    "# Samples validation,Training epochs,Pretrained type,Pretrained model path,\n",
    "Loss function,Best epoch validation Loss,\n",
    "Train Loss (From best validation epoch), Best validation epoch,Comment\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "\n",
    "def write_modelSummary_row(modelSummary_folder, row_data):\n",
    "    \n",
    "    if not Path(modelSummary_folder).is_dir():\n",
    "        print(\"Creating modelSummary folder...\")\n",
    "        os.mkdir(modelSummary_folder)\n",
    "        \n",
    "    print(\"Trying to open file...\")\n",
    "    try: \n",
    "        file = open(modelSummary_folder+\"/model_summary.csv\", \"r+\")\n",
    "        # check header\n",
    "        first_line = next(file).replace('\\n', '')\n",
    "        assert first_line==header.replace('\\n', '')\n",
    "\n",
    "        file.writelines('\\n'+row_data)\n",
    "\n",
    "    except IOError:\n",
    "        print(\"No file found, writing header and row_data...\")\n",
    "        file = open(modelSummary_folder+\"/model_summary.csv\", \"a+\")\n",
    "        file.writelines([header.replace('\\n', ''), \"\\n\"+row_data])\n",
    "\n",
    "    finally:\n",
    "        file.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "def write_modelSummary(trainer, modelSummary_folder, path_data, path_target, rsample_train, rsample_val):\n",
    "    p = Path(trainer.tensorboard_logdir+'default/')\n",
    "    p = [f.as_posix() for f in list(p.glob('*'))]\n",
    "    last_version = sorted([int(f.split('version_')[-1]) for f in p])[-1]\n",
    "    last_version = 'v'+ str(last_version)\n",
    "    \n",
    "    print(trainer.model.val_loss_per_epoch, trainer.model.train_loss_per_epoch)\n",
    "    \n",
    "    best_val_loss_epoch = np.min(trainer.model.val_loss_per_epoch)\n",
    "    best_val_loss_epoch_idx = np.argmin(trainer.model.val_loss_per_epoch)\n",
    "    # pytorch lightning includes epoch zero (no training) in the validation\n",
    "    associated_train_loss_epoch = trainer.model.train_loss_per_epoch[best_val_loss_epoch_idx-1]\n",
    "    \n",
    "    tomograms = [f.split('/')[-1] for f in path_data]\n",
    "    tomograms = np.array([f.split('_')[0] for f in tomograms])\n",
    "\n",
    "    _tomograms = [f.split('/')[-1] for f in path_target]\n",
    "    _tomograms = np.array([f.split('_')[0] for f in _tomograms])\n",
    "\n",
    "    assert all(tomograms == _tomograms)\n",
    "\n",
    "    train_set = np.unique([x['tomo_idx'] for x in rsample_train])\n",
    "    train_set = ' '.join(tomograms[train_set])\n",
    "\n",
    "    validation_set = np.unique([x['tomo_idx'] for x in rsample_val])\n",
    "    validation_set = ' '.join(tomograms[validation_set])\n",
    "    \n",
    "    \n",
    "    row_data = [trainer.model_name, last_version, train_set, len(rsample_train), validation_set, len(rsample_val), trainer.epochs,\n",
    "                trainer.pretrain_type, trainer.pretrained_model, trainer.loss_fn, best_val_loss_epoch, float(associated_train_loss_epoch),\n",
    "                best_val_loss_epoch_idx]\n",
    "    \n",
    "    row_data =','.join([str(x) for x in row_data])\n",
    "    write_modelSummary_row(modelSummary_folder, row_data)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "15e68e3e-6791-4af0-a18a-5c4be9c4e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1.0307), tensor(0.9499), tensor(0.5043)] [tensor(1.0041), tensor(0.7415)]\n",
      "Trying to open file...\n"
     ]
    }
   ],
   "source": [
    "write_modelSummary(trainer, save_folder, path_data, path_target, rsample_train, rsample_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a7a6bfe-ffe8-4526-9781-48eb2f4856ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tomo10, tomo32'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae7f65c4-2fa5-4cec-89de-e75daa81e289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ae77403-170e-47e5-a01a-224683a4e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51f05d5c-1792-4e8b-aa08-678ea94fa4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing tomo12']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r'^tomo\\d+')\n",
    "p.findall('tomo12 drummers drumming, 11 pipers piping, 10 lords a-leaping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541bd73c-8000-4081-b754-9a2adbf64114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
