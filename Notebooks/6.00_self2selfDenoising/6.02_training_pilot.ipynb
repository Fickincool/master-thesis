{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d66c41-29e5-48ef-8456-94c15737302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from tomoSegmentPipeline.utils.common import read_array, write_array\n",
    "from tomoSegmentPipeline.utils import setup\n",
    "\n",
    "from cryoS2Sdrop.dataloader import singleCET_dataset\n",
    "from cryoS2Sdrop.model import Denoising_UNet\n",
    "from cryoS2Sdrop.losses import self2self_L2Loss\n",
    "from cryoS2Sdrop.trainer import denoisingTrainer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "PARENT_PATH = setup.PARENT_PATH\n",
    "ISONET_PATH = os.path.join(PARENT_PATH, 'data/isoNet/')\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a05be57a-7092-48b0-8450-891560affde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cet_path = os.path.join(PARENT_PATH, 'data/raw_cryo-ET/tomo02.mrc') \n",
    "cet_path = os.path.join(PARENT_PATH, 'data/S2SDenoising/dummy_tomograms/tomo02_dummy.mrc')\n",
    "p=0.3 # dropout probability\n",
    "subtomo_length = 128\n",
    "n_bernoulli_samples = 4\n",
    "n_features = 48\n",
    "tensorboard_logdir = os.path.join(PARENT_PATH, 'data/S2SDenoising/tryout_model_logs')\n",
    "batch_size = 4\n",
    "epochs = 10\n",
    "num_gpus = 2\n",
    "lr = 1e-4\n",
    "\n",
    "s2s_trainer = denoisingTrainer(cet_path, subtomo_length, lr, n_bernoulli_samples, n_features, p, tensorboard_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d22e26-f6d6-40e7-b5b1-baf30850b4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name      | Type             | Params\n",
      "------------------------------------------------\n",
      "0  | loss_fn   | self2self_L2Loss | 0     \n",
      "1  | EB1       | PartialConv3d    | 1.3 K \n",
      "2  | EB2       | Sequential       | 62.3 K\n",
      "3  | EB3       | Sequential       | 62.3 K\n",
      "4  | EB4       | Sequential       | 62.3 K\n",
      "5  | EB5       | Sequential       | 62.3 K\n",
      "6  | EB6       | Sequential       | 62.3 K\n",
      "7  | EB_bottom | Sequential       | 62.3 K\n",
      "8  | up65      | Upsample         | 0     \n",
      "9  | up54      | Upsample         | 0     \n",
      "10 | up43      | Upsample         | 0     \n",
      "11 | up32      | Upsample         | 0     \n",
      "12 | up21      | Upsample         | 0     \n",
      "13 | DB5       | Sequential       | 497 K \n",
      "14 | DB4       | Sequential       | 622 K \n",
      "15 | DB3       | Sequential       | 622 K \n",
      "16 | DB2       | Sequential       | 622 K \n",
      "17 | DB1       | Sequential       | 223 K \n",
      "------------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "11.854    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 32, Steps per epoch: 4. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric hp/train_loss improved. New best score: 794.893\n",
      "Metric hp/train_loss improved by 0.996 >= min_delta = 0.0001. New best score: 793.897\n",
      "Metric hp/train_loss improved by 2.825 >= min_delta = 0.0001. New best score: 791.072\n",
      "Metric hp/train_loss improved by 4.572 >= min_delta = 0.0001. New best score: 786.501\n",
      "Metric hp/train_loss improved by 3.518 >= min_delta = 0.0001. New best score: 782.983\n",
      "Metric hp/train_loss improved by 2.423 >= min_delta = 0.0001. New best score: 780.560\n"
     ]
    }
   ],
   "source": [
    "s2s_trainer.train(batch_size, epochs, num_gpus, accelerator = 'gpu', strategy = 'dp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec202e-9b05-4a84-8927-8b3fd0d763ef",
   "metadata": {},
   "source": [
    "# Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c27cd7da-4414-45f1-834b-409fdbd1abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = os.path.join(PARENT_PATH, 'data/S2SDenoising/tryout_model_logs/version_0/checkpoints/epoch=1-step=126.ckpt')\n",
    "\n",
    "model = Denoising_UNet.load_from_checkpoint(ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a668c9f-71f6-4b01-b6f8-871de00d089d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
